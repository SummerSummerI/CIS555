HW2MS3
======



##Steps
- Using BFS to crawl the links in the sandbox
- Using BerkeleyDB to store the crawled webpages
  - Take a look at the examples coming with the downloaded package.
  - Modify and trim the example to our needs
    - Load ==> Store.put
    - Read ==> Store.get
    - Binding
    - MyDbEnv
  - Design a web page data model to store the content and some other useful information
  - Setup
- Politeness
  - Read the robots.txt file
  - Parse into hashtables
  - Follow specific rules when crawling the web
